/*â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
  â•‘  â–‘  M I N D T Y P E   D E M O  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â•‘
  â•‘                                                              â•‘
  â•‘   Command-line demo of the correction pipeline.             â•‘
  â•‘                                                              â•‘
  â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
*/

import Foundation
import MindTypeCore

@main
struct MindTypeDemo {
    static func main() async {
        print("""
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘           M I N D â ¶ T Y P E   D E M O   v 1 . 0              â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        Testing the three-stage correction pipeline:
        â€¢ Noise  â†’ Fix typos
        â€¢ Context â†’ Improve grammar  
        â€¢ Tone   â†’ Adjust style
        
        """)
        
        // Try to use real LLM if model available, otherwise fall back to mock
        let adapter: any LMAdapter
        let usingRealLM: Bool
        
        if let modelPath = ModelDiscovery.findModel() {
            print("ğŸ§  Found model: \(modelPath)")
            let llamaAdapter = LlamaLMAdapter()
            do {
                try await llamaAdapter.initialize(config: .gguf(modelPath))
                adapter = llamaAdapter
                usingRealLM = true
                print("âœ… Llama adapter initialized (Metal-accelerated)")
            } catch {
                print("âš ï¸  Failed to load model: \(error.localizedDescription)")
                print("   Falling back to mock adapter...")
                adapter = MockLMAdapter()
                usingRealLM = false
                try? await (adapter as! MockLMAdapter).initialize(config: .default)
            }
        } else {
            print("â„¹ï¸  No GGUF model found. Using mock adapter.")
            print("   To use real LLM, download model to: ~/.mindtype/models/")
            print("   curl -L -o ~/.mindtype/models/\(ModelDiscovery.defaultModelName) \\")
            print("     \(ModelDiscovery.downloadURL)")
            print("")
            let mockAdapter = MockLMAdapter()
            try? await mockAdapter.initialize(config: .default)
            adapter = mockAdapter
            usingRealLM = false
        }
        
        print("")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("  Mode: \(usingRealLM ? "ğŸš€ Real LLM (Qwen 0.5B)" : "ğŸ­ Mock (pattern matching)")")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        let pipeline = CorrectionPipeline(lmAdapter: adapter)
        
        // Test cases
        let testCases: [(text: String, description: String)] = [
            ("I was writting a letter to my freind becuase I beleive its neccessary.", "Multiple typos"),
            ("Teh quick brown fox jumps over teh lazy dog.", "Common transpositions"),
            ("This is definately wierd but I cant help it.", "Mixed typos"),
            ("I recieved teh message tommorow.", "Various corrections"),
        ]
        
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        
        for (index, testCase) in testCases.enumerated() {
            print("\nğŸ“ Test \(index + 1): \(testCase.description)")
            print("   Input:  \"\(testCase.text)\"")
            
            do {
                let result = try await pipeline.runCorrectionWave(
                    text: testCase.text,
                    caret: testCase.text.count
                )
                
                if result.diffs.isEmpty {
                    print("   Output: (no changes)")
                } else {
                    // Apply corrections safely using applyDiffs
                    if let applied = applyDiffs(
                        text: testCase.text,
                        diffs: result.diffs,
                        caret: testCase.text.count
                    ) {
                        print("   Output: \"\(applied.text)\"")
                    } else {
                        print("   Output: (diffs failed to apply)")
                    }
                    print("   â±ï¸  Latency: \(String(format: "%.1f", result.durationMs)) ms")
                    print("   ğŸ“Š Corrections: \(result.diffs.count)")
                    for diff in result.diffs {
                        print("      â€¢ [\(diff.stage.displayName)] [\(diff.start):\(diff.end)] â†’ \"\(diff.text)\"")
                    }
                }
            } catch {
                print("   âŒ Error: \(error)")
            }
        }
        
        print("\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("\nâœ… Demo complete!")
        
        if usingRealLM {
            print("""
            
            ğŸ‰ Running with real on-device LLM!
            
            The Qwen 0.5B model is providing intelligent corrections via Metal.
            Next steps:
            â€¢ Integrate with macOS Accessibility APIs for system-wide corrections
            â€¢ Use the MindTypeUI components for visual feedback
            â€¢ Fine-tune temperature/prompts for your use case
            
            """)
        } else {
            print("""
            
            The mock adapter demonstrates the pipeline architecture.
            To enable real LLM inference:
            
            1. Download the model (~394MB):
               curl -L -o ~/.mindtype/models/\(ModelDiscovery.defaultModelName) \\
                 '\(ModelDiscovery.downloadURL)'
            
            2. Re-run the demo:
               swift run MindTypeDemo
            
            """)
        }
    }
}

